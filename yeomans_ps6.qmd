---
title: "Problem Set 6"
author: "Sydney Yeomans"
format:
  html:
    embed-resources: true
editor: visual
---


## Problem 1

We defined a C_mean function. Using this as a template, implement a C_moment function that returns the kth central moment.


```{r, error=TRUE}
#| echo: true
#library(Rcpp)
#template
#cppFunction("
#double C_mean(NumericVector v) {
#  double sum = 0;
#  for (int i = 0; i < v.length(); ++i){
#     sum += v[i];
#  }
#  return(sum/v.length());
#}")

#moment formula: mu = E[(x-mu)^k]
cppFunction("
double C_moment(NumericVector v, int k) {
  int n = v.length();
  
  // compute the mean, copt from template
  double sum = 0;
  for (int i = 0; i < v.length(); ++i) {
    sum += v[i];
  }
  double mean = sum / v.length();

  // moment calculation
  double mom_sum = 0;
  for (int i = 0; i < v.length(); ++i) {
    mom_sum += pow(v[i] - mean, k);
  }

  return mom_sum / v.length();
}
")
```

Generate a vector of moderate length and show that you are able to replicate the results of e1071::moment.
 
```{r}
#Be cognizant of your scaling factor.
#Be sure to look at the arguments of e1071::moment
#install.packages("e1071")
library(e1071)
#?e1071::moment

set.seed(3)
x <- rnorm(50)
C_moment(x, 4)
moment(x, order = 4, center = TRUE, absolute = FALSE, scale = TRUE)
```

I ran into issues constantly with this problem, issues on a system level not really with the code itself. I keep getting the error to install something to compile C code, but I have installed it (and uninstalled it) multiple times and it has never worked. I am assuming this would work if whatever underlying C/C++ issues I am running into was not occurring. I have given up on trying to get it to work.

## Problem 2

a. Write a class bootstrapWaldCI that produces a CI using bootstrap, similar to waldCI.

```{r}
#library(parallel)
source("waldCI_sol.R")
```

```{r}
setClass("bootstrapWaldCI", contains = "waldCI", 
         slots = c(fun = "function",
                   data = "data.frame",
                   reps = "numeric",
                   bootEstimates = "numeric",
                   compute = "character"))

makeBootstrapCI <- function(fun, data, reps = 1000, level = 0.95, 
                            compute = "serial"){
  if (compute == "serial") {
    
    bootstrap_est <- lapply(seq_len(reps), function(i) {
      dat <- data[sample(seq_len(nrow(data)), replace = TRUE), ]
      return(fun(dat))
      
    })
  } else if (compute == "parallel") {
    
    cl <- makeCluster(detectCores() - 1)   
    clusterExport(cl, c("data", "fun"), envir = environment())
    bootstrap_est <- parLapply(cl, seq_len(reps), function(i) {
    dat <- data[sample(seq_len(nrow(data)), replace = TRUE), ]
    return(fun(dat))
    
    })

    stopCluster(cl)
  }
  boot_ans <- unlist(bootstrap_est)

  boot_mean <- mean(boot_ans)
  boot_se <- sd(boot_ans)
  
  return(new("bootstrapWaldCI", fun = fun, data = data, reps = reps,
         level = level, mean = boot_mean, sterr = boot_se, bootEstimates = boot_ans,
         compute = compute))
}
```

```{r}
setGeneric("rebootstrap",
           function(object) {
             standardGeneric("rebootstrap")
           })

setMethod("rebootstrap", "bootstrapWaldCI", function(object) {
  fun <- object@fun
  data <- object@data
  reps <- object@reps
  compute <- object@compute
  level <- object@level
  if (compute == "serial") {
    
    bootstrap_est <- lapply(seq_len(reps), function(i) {
      dat <- data[sample(seq_len(nrow(data)), replace = TRUE), ]
      return(fun(dat))
      
    })
  } else if (compute == "parallel") {
    
    cl <- makeCluster(detectCores() - 1)   
    clusterExport(cl, c("data", "fun"), envir = environment())
    bootstrap_est <- parLapply(cl, seq_len(reps), function(i) {
    dat <- data[sample(seq_len(nrow(data)), replace = TRUE), ]
    return(fun(dat))
    
    })

    stopCluster(cl)
  }
  boot_ans <- unlist(bootstrap_est)

  boot_mean <- mean(boot_ans)
  boot_se <- sd(boot_ans)
  
  return(new("bootstrapWaldCI", fun = fun, data = data, reps = reps,
         level = level, mean = boot_mean, sterr = boot_se, bootEstimates = boot_ans,
         compute = compute))
})
```


b. Show your code works by executing the following:

```{r}
ci1 <- makeBootstrapCI(function(x) mean(x$y),
                       ggplot2::diamonds,
                       reps = 1000)
ci1
rebootstrap(ci1)
```

```{r}
system.time({
  ci_serial <- makeBootstrapCI(function(x) mean(x$y),
                               ggplot2::diamonds,
                               reps = 1000,
                               compute = "serial")
})

system.time({
  ci_parallel <- makeBootstrapCI(function(x) mean(x$y),
                                 ggplot2::diamonds,
                                 reps = 1000,
                                 compute = "parallel")
})
```

The two CIs are very close to each other and we can see things are running well.

By comparing the two run time of the compute methods, serial and parallel, we see something quite interesting. The parallel actually took more time to run than the serial one. I think this is due to the fact that I am on Windows. On Windows, parallelization requires creating separate R processes and exporting objects to them, so it takes longer than doing it serially.


c. Write a function called dispCoef that takes in data (based upon mtcars; it must take in a generic data for the bootstrap) and fits the model: mpg ~ cyl + disp + wt. It should return the coefficient associated with disp. 

```{r}
dispCoef <- function(data) {
  model <- lm(mpg ~ cyl + disp + wt, data = data)
  coef(model)["disp"]
}
ci2 <- makeBootstrapCI(dispCoef,
                       mtcars,
                       reps = 1000)
ci2
rebootstrap(ci2)
```
Got about the same CIs for the two methods!

```{r}
system.time({
  ci_serial <- makeBootstrapCI(dispCoef,
                       mtcars,
                       reps = 1000,
                       compute = "serial")
})

system.time({
  ci_parallel <- makeBootstrapCI(dispCoef,
                       mtcars,
                       reps = 1000,
                       compute = "parallel")
})
```

Once again the parallel takes more time than the serial, due to the fact that this is a small job so sending it off to different cores and coming back actually takes longer than just running it in serial.



## Problem 3

Generate artificial data by running the code in script:

```{r}
source("script_q3.R")
```

a. Fit one model per country. Fit a mixed effects logistic regression model, predicting course completion based upon prior GPA, number of forum posts, number of quiz attempts, and a random effect for device type. Standardize the predictors within each country. Generate some sort of visualization of the estimated coefficients for number of forum posts in each country.

```{r}
#| echo: true
#Germany
df_germ <- df %>%
           filter(country == "Germany") %>%
           mutate(prior_gpa_s = scale(prior_gpa),
                  forum_posts_s = scale(forum_posts),
                  quiz_attempts_s = scale(quiz_attempts)) 

lr_germany <- glmer(completed_course ~ prior_gpa_s + 
                                       forum_posts_s + 
                                       quiz_attempts_s + 
                                       (1|device_type), data = df_germ,
                                       family = binomial) 
lr_germany

#united states
df_us <- df %>%
         filter(country == "US") %>%
         mutate(prior_gpa_s = scale(prior_gpa),
                forum_posts_s = scale(forum_posts),
                quiz_attempts_s = scale(quiz_attempts))

lr_us <- glmer(completed_course ~ prior_gpa_s + 
                                  forum_posts_s + 
                                  quiz_attempts_s + 
                                  (1|device_type), data = df_us,
                                  family = binomial) 
lr_us

#India
df_india <- df %>%
            filter(country == "India") %>%
            mutate(prior_gpa_s = scale(prior_gpa),
                   forum_posts_s = scale(forum_posts),
                   quiz_attempts_s = scale(quiz_attempts))

lr_india <- glmer(completed_course ~ prior_gpa_s + 
                                     forum_posts_s + 
                                     quiz_attempts_s + 
                                     (1|device_type), data = df_india,
                                     family = binomial) 
lr_india

#Lithuania
df_lithuania <- df %>%
                filter(country == "Lithuania") %>%
                mutate(prior_gpa_s = scale(prior_gpa),
                       forum_posts_s = scale(forum_posts),
                       quiz_attempts_s = scale(quiz_attempts)) 

lr_lithuania <- glmer(completed_course ~ prior_gpa_s + 
                                         forum_posts_s + 
                                         quiz_attempts_s + 
                                         (1|device_type), data = df_lithuania,
                                         family = binomial) 
lr_lithuania

#Nigeria
df_nigeria <- df %>%
              filter(country == "Nigeria") %>%
              mutate(prior_gpa_s = scale(prior_gpa),
                     forum_posts_s = scale(forum_posts),
                     quiz_attempts_s = scale(quiz_attempts)) 

lr_nigeria <- glmer(completed_course ~ prior_gpa_s + 
                                       forum_posts_s + 
                                       quiz_attempts_s + 
                                       (1|device_type), data = df_nigeria,
                                       family = binomial) 
lr_nigeria

#other countries
df_other <- df %>%
            filter(country == "Other") %>%
            mutate(prior_gpa_s = scale(prior_gpa),
                   forum_posts_s = scale(forum_posts),
                   quiz_attempts_s = scale(quiz_attempts)) 

lr_other <- glmer(completed_course ~ prior_gpa_s + 
                                     forum_posts_s + 
                                     quiz_attempts_s + 
                                     (1|device_type), data = df_other,
                                     family = binomial)
lr_other
```


```{r}
#| echo: true
#Visualization of the estimated coefficients for number of forum posts in each country
#get the coefficients first and make a data frame with them
forum_posts_coefs <- data.frame(
  country = c("Germany", "US", "India", "Lithuania", "Nigeria", "Other"),
  coef = c(0.1766, 0.1782, 0.1762, 0.1747, 0.1916, 0.1751))

# Plot
ggplot(forum_posts_coefs, aes(x = country, y = coef)) +
  geom_point(color = "navy") +
  labs(title = "Coefficient for the Forum Posts, by Country",
       x = "Country",
       y = "Coefficient") +
  theme_bw()
```
From this graph we can see that all the countries have similar coefficients for the forum posts. However, we do see a bit of a gap between Nigeria and the others. Also, have to note that the coefficients are the log of the odds ratio and kind of hard to interpret. All the coefficients are in the range of $0.1700$ and $0.200$.

Report the running time (from system.time) for each of the 6 models.

```{r}
time_germ <- system.time({
  lr_germany <- glmer(completed_course ~ prior_gpa_s + 
                                       forum_posts_s + 
                                       quiz_attempts_s + 
                                       (1|device_type), data = df_germ,
                                       family = binomial) 
})
time_germ["elapsed"]
  
time_ind <- system.time({
  lr_india <- glmer(completed_course ~ prior_gpa_s + 
                                     forum_posts_s + 
                                     quiz_attempts_s + 
                                     (1|device_type), data = df_india,
                                     family = binomial) 
})
time_ind["elapsed"]

time_us <- system.time({
  lr_us <- glmer(completed_course ~ prior_gpa_s + 
                                  forum_posts_s + 
                                  quiz_attempts_s + 
                                  (1|device_type), data = df_us,
                                  family = binomial) 
})
time_us["elapsed"]

time_lith <- system.time({
  lr_lithuania <- glmer(completed_course ~ prior_gpa_s + 
                                         forum_posts_s + 
                                         quiz_attempts_s + 
                                         (1|device_type), data = df_lithuania,
                                         family = binomial) 
})
time_lith["elapsed"]

time_nigeria <- system.time({
  lr_nigeria <- glmer(completed_course ~ prior_gpa_s + 
                                       forum_posts_s + 
                                       quiz_attempts_s + 
                                       (1|device_type), data = df_nigeria,
                                       family = binomial) 
})
time_nigeria["elapsed"]

time_other <- system.time({
  lr_other <- glmer(completed_course ~ prior_gpa_s + 
                                     forum_posts_s + 
                                     quiz_attempts_s + 
                                     (1|device_type), data = df_other,
                                     family = binomial) 
})
time_other["elapsed"]

```
From this, we can see that all the models take a while to run, but we see that the other category (321), India (182), and the US (228) take a bit longer than the others (possibly due to them having more observations, particularly for other bc this could have multiple countries. On the other hand, Lithuania (2) and Nigeria (5) take the shortest time to run with only a couple seconds(I assume this is the units), maybe because it has less observations. This has a total run time of 860.57.

b. Devise an approach that minimizes the running time of your script. Report the running time of your entire script (running models and estimating coefficients; no need for a new plot). Do not use a different package for the models. Show that the results match those from part a).

This doesn't say I have to do parallel, so I am just going to do lapply here. Sorry but I really don't want to do more parallel on my Windows machine (which is old and slow anyway).

```{r}
#filter everything for the countries at once
countries <- c("Germany","US","India","Lithuania","Nigeria","Other")
df_faster <- lapply(countries, function(ctry) {
              df %>%
              filter(country == ctry) %>%
              mutate(prior_gpa_s = scale(prior_gpa),
                     forum_posts_s = scale(forum_posts),
                     quiz_attempts_s = scale(quiz_attempts))
 })
 
models_glmer <- function(data) {
   glmer(
     completed_course ~ prior_gpa_s + forum_posts_s + quiz_attempts_s + (1|device_type),
     data = data,
     family = binomial
   )
 }
 
cl <- makeCluster(detectCores() - 1)   
clusterEvalQ(cl, library(lme4))        
clusterExport(cl, c("df_faster", "models_glmer"))  

time_parallel <- system.time({
  model_list <- parLapply(cl, df_faster, models_glmer)
})

stopCluster(cl)

time_parallel["elapsed"]
```
Doing parallelization shaved off about 400 seconds of runtime which is kinda crazy. My computer did sound like a jet engine when running this but that's a different story. The total run time using parallelization was 425.36 seconds but I didn't do the individual model runtimes. I don't think this matters too much, just running the models in parallel really saved a lot of time. I will say I was struggling with this quite a bit and did lots of different tries without the parallel. I ended up using Chat to help figure out how to get everything to work properly on my windows laptop but only after I had a base going (it mostly helped with how many cores to use as well as eval and export since I'm not too confident in that). Thanks to this I was able to cut a lot of time off. When I was doing lapply stuff and other techniques it, at max, shaved off 80-100 seconds only. 


## Problem 4

Redo of problem set 4 question 2 with data table.

```{r}
atp <- fread("https://raw.githubusercontent.com/JeffSackmann/tennis_atp/refs/heads/master/atp_matches_2019.csv")
class(atp)
```

a. How many tournaments took place in 2019?

```{r}
#exclude the couple dates from 2018 and condense the davis cup (from solutions
#I didn't do this last time)
atp <- atp[tourney_date != "20181231"]
tournaments <- atp[, .(tourney_name = unique(tourney_name))]

#this is from the solutions and what i need to do for davis situation
#tourneys <- tourneys %>%
#  mutate(tourney_name = str_replace(tourney_name, "Davis.*", "Davis Cup")) %>%
#  group_by(tourney_name) %>%
#  summarize() %>%
#  ungroup()
#tourneys %>%
#  nrow()

#davis fix
fixed_davis <- tournaments[
  , tourney_name := str_replace(tourney_name, "Davis.*", "Davis Cup")
  ][, .(tourney_name = unique(tourney_name))]

fixed_davis[, .N]

```
This doesn't quite match the solutions, but it makes sense it is off by three. This is due to the fact that I am still excluding the 2018 date, in which there are three observations. The solutions didn't include this, so it makes sense mine is off by $3$ for a total number of $66$ unique tournaments.

b. Did any player win more than one tournament? If so, how many players won more than one tournament, and how many tournaments did the most winning player(s) win?

```{r}
play_win <- atp[, .SD[which.max(match_num)], by = tourney_name]

multiple_W <- play_win[, .(wins = .N), by = winner_name][wins > 1][order(-wins)]

multiple_W[, .N]     
multiple_W[1:2]

```
The total number of players with multiple tournament wins is $17$ and the player with the most wins was Rafael Nadal with $9$ tournament wins, which matches the solutions. 

c. Is there any evidence that winners have more aces than losers? (If you address this with a hypothesis test, do not use base R functionality - continue to remain in the Tidyverse.)

```{r}
aces <- atp[, .(mean_winners = mean(w_ace, na.rm = TRUE),
                mean_losers = mean(l_ace, na.rm = TRUE),
                sd_winners = sd(w_ace, na.rm = TRUE),
                sd_losers = sd(l_ace, na.rm = TRUE),
            mean_diff = mean(w_ace, na.rm = TRUE) - mean(l_ace, na.rm = TRUE))]
aces
```
I decided to just stick with doing some summary statistics as I did in problem set $4$. From this, we see that the mean aces for winners is $7.45$, rounded to two decimal places. On the other hand, the mean aces for losers is $5.76$, rounded to two decimal places. After finding this and the standard deviation from both, we took the difference in means and found the difference was $1.68$, so we can, informally, say winners on average have more aces than losers. Also, the standard deviations for both groups were quite close meaning the groups had similar variability.

d. Identify the player(s) with the highest win-rate. (Note that this is NOT asking for the highest number of wins.) Restrict to players with at least 5 matches.

```{r}
wins_tot <- atp[ , .(num_wins = .N), 
                by = .(player_id = winner_id, player_name = winner_name)]

loss_tot <- atp[ , .(num_losses = .N), 
                by = .(player_id = loser_id, player_name = loser_name)]

total_players <- merge(wins_tot, loss_tot, by = c("player_id", "player_name"), 
                      all = TRUE)
#deal with NAs just like last time
total_players[is.na(num_wins), num_wins := 0]
total_players[is.na(num_losses), num_losses := 0]
#add columns for total matches and the win rate
total_players[, total_matches := num_wins + num_losses]
total_players[, win_rate := (num_wins / total_matches) * 100]
#Restrict to players with at least 5 matches and start with highest
win_rate_final <- total_players[total_matches >= 5, .(player_name, win_rate)
                                ][order(-win_rate)]
head(win_rate_final)
```
From this, we can see that Rafael Nadal has the highest win rate with $86.96\%$ (rounded to two decimal places), and he was the player with the most wins as well. This matches the solutions from problem set 4. 


